groups:
  - name: fedora_workstation_alerts
    rules:
      # 1. Is de machine/exporter bereikbaar?
      - alert: InstanceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Instance {{ $labels.instance }} is down"
          description: "De node exporter is al meer dan 1 minuut niet bereikbaar."

      # 2. Waarschuwing als Disk ruimte < 10% is
      - alert: HostOutOfDiskSpace
        expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Disk bijna vol ({{ $labels.instance }})"
          description: "Nog maar {{ $value | humanize }}% ruimte over op partitie {{ $labels.mountpoint }}."

      # 3. Hoge CPU belasting (als load hoger is dan aantal cores * 1.5)
      - alert: HostHighCpuLoad
        expr: node_load1 > (count without (cpu) (node_cpu_seconds_total{mode="idle"})) * 1.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Hoge CPU Load ({{ $labels.instance }})"
          description: "CPU load is al 5 minuten extreem hoog."

      # 4. Geheugen vol (< 10% beschikbaar)
      - alert: HostOutOfMemory
        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Geheugen bijna vol ({{ $labels.instance }})"
          description: "RAM geheugen is bijna op (< 10% vrij)."
      
      # 5. Geheugen druk (hoge page faults)
      - alert: HostMemoryUnderMemoryPressure
        expr: (rate(node_vmstat_pgmajfault[5m]) > 1000)
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Host memory under memory pressure (instance {{ $labels.instance }})
          description: "The node is under heavy memory pressure. High rate of loading memory pages from disk.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 6. You may want to increase the alert manager 'repeat_interval' for this type of alert to daily or weekly
      - alert: HostMemoryIsUnderutilized
        expr: min_over_time(node_memory_MemFree_bytes[1w]) > node_memory_MemTotal_bytes * .8
        for: 0m
        labels:
          severity: info
        annotations:
          summary: Host Memory is underutilized (instance {{ $labels.instance }})
          description: "Node memory usage is < 20% for 1 week. Consider reducing memory space. (instance {{ $labels.instance }})\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 7. Hoge netwerk doorvoer uitgaand (>80% van de netwerk snelheid)
      - alert: HostUnusualNetworkThroughputIn
        expr: ((rate(node_network_receive_bytes_total[5m]) / node_network_speed_bytes) > .80)
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Host unusual network throughput in (instance {{ $labels.instance }})
          description: "Host receive bandwidth is high (>80%).\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 8. Hoge netwerk doorvoer inkomend (>80% van de netwerk snelheid)
      - alert: HostUnusualNetworkThroughputOut
        expr: ((rate(node_network_transmit_bytes_total[5m]) / node_network_speed_bytes) > .80)
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Host unusual network throughput out (instance {{ $labels.instance }})
          description: "Host transmit bandwidth is high (>80%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 9. Hoge disk schrijf snelheid (>80% van de disk snelheid)
      - alert: HostUnusualDiskReadRate
        expr: (rate(node_disk_io_time_seconds_total[5m]) > .80)
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Host unusual disk read rate (instance {{ $labels.instance }})
          description: "Disk is too busy (IO wait > 80%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 10. Kritieke waarschuwing als Disk ruimte < 10% is
      # Please add ignored mountpoints in node_exporter parameters like
      # "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)".
      # Same rule using "node_filesystem_free_bytes" will fire when disk fills for non-root users.
      - alert: HostOutOfDiskSpace
        expr: (node_filesystem_avail_bytes{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"} / node_filesystem_size_bytes < .10 and on (instance, device, mountpoint) node_filesystem_readonly == 0)
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: Host out of disk space (instance {{ $labels.instance }})
          description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 11. Kritieke waarschuwing als Disk ruimte op raakt binnen 24 uur
      # Please add ignored mountpoints in node_exporter parameters like
      # "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)".
      # Same rule using "node_filesystem_free_bytes" will fire when disk fills for non-root users.
      - alert: HostDiskMayFillIn24Hours
        expr: predict_linear(node_filesystem_avail_bytes{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"}[3h], 86400) <= 0 and node_filesystem_avail_bytes > 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host disk may fill in 24 hours (instance {{ $labels.instance }})
          description: "Filesystem will likely run out of space within the next 24 hours.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 12. Kritieke waarschuwing als inodes op raken (< 10% left)
      - alert: HostOutOfInodes
        expr: (node_filesystem_files_free / node_filesystem_files < .10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0)
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: Host out of inodes (instance {{ $labels.instance }})
          description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 13. Kritieke waarschuwing als er een device error is opgetreden
      - alert: HostFilesystemDeviceError
        expr: node_filesystem_device_error{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"} == 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: Host filesystem device error (instance {{ $labels.instance }})
          description: "Error stat-ing the {{ $labels.mountpoint }} filesystem\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 14. Kritieke waarschuwing als inodes op raken binnen 24 uur
      - alert: HostInodesMayFillIn24Hours
        expr: predict_linear(node_filesystem_files_free{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"}[1h], 86400) <= 0 and node_filesystem_files_free > 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host inodes may fill in 24 hours (instance {{ $labels.instance }})
          description: "Filesystem will likely run out of inodes within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 15. Waarschuwing bij ongebruikelijke disk write latency (> 100ms)
      - alert: HostUnusualDiskReadLatency
        expr: (rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0)
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host unusual disk read latency (instance {{ $labels.instance }})
          description: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 16. Waarschuwing bij ongebruikelijke disk write latency (> 100ms)
      - alert: HostUnusualDiskWriteLatency
        expr: (rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.1 and rate(node_disk_writes_completed_total[1m]) > 0)
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host unusual disk write latency (instance {{ $labels.instance }})
          description: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 17. Waarschuwing bij hoge CPU load (> 80% gebruikt)
      - alert: HostHighCpuLoad
        expr: 1 - (avg without (cpu) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) > .80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: Host high CPU load (instance {{ $labels.instance }})
          description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 18. You may want to increase the alert manager 'repeat_interval' for this type of alert to daily or weekly
      # You may want to increase the alert manager 'repeat_interval' for this type of alert to daily or weekly
      - alert: HostCpuIsUnderutilized
        expr: (min without (cpu) (rate(node_cpu_seconds_total{mode="idle"}[1h]))) > 0.8
        for: 1w
        labels:
          severity: info
        annotations:
          summary: Host CPU is underutilized (instance {{ $labels.instance }})
          description: "CPU load has been < 20% for 1 week. Consider reducing the number of CPUs.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 19. Waarschuwing bij hoge CPU steal (> 10%)
      - alert: HostCpuStealNoisyNeighbor
        expr: avg without (cpu) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Host CPU steal noisy neighbor (instance {{ $labels.instance }})
          description: "CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 20. Waarschuwing bij hoge CPU iowait (> 10%)
      - alert: HostCpuHighIowait
        expr: avg without (cpu) (rate(node_cpu_seconds_total{mode="iowait"}[5m])) > .10
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Host CPU high iowait (instance {{ $labels.instance }})
          description: "CPU iowait > 10%. Your CPU is idling waiting for storage to respond.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 21. Waarschuwing bij ongebruikelijke disk IO (> 80% busy)
      - alert: HostUnusualDiskIo
        expr: rate(node_disk_io_time_seconds_total[5m]) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Host unusual disk IO (instance {{ $labels.instance }})
          description: "Disk usage >80%. Check storage for issues or increase IOPS capabilities. Check storage for issues.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 22. Waarschuwing bij hoge context switching (> 2x dagelijkse gemiddelde)
      # x2 context switches is an arbitrary number.
      # The alert threshold depends on the nature of the application.
      # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58
      - alert: HostContextSwitchingHigh
        expr: (rate(node_context_switches_total[15m])/count without(mode,cpu) (node_cpu_seconds_total{mode="idle"})) / (rate(node_context_switches_total[1d])/count without(mode,cpu) (node_cpu_seconds_total{mode="idle"})) > 2
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Host context switching high (instance {{ $labels.instance }})
          description: "Context switching is growing on the node (twice the daily average during the last 15m)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 23. Waarschuwing als swap ruimte voor meer dan 80% gebruikt is
      - alert: HostSwapIsFillingUp
        expr: ((1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80)
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host swap is filling up (instance {{ $labels.instance }})
          description: "Swap is filling up (>80%)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 24. Waarschuwing als een systemd service is gecrasht
      - alert: HostSystemdServiceCrashed
        expr: (node_systemd_unit_state{state="failed"} == 1)
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Host systemd service crashed (instance {{ $labels.instance }})
          description: "systemd service crashed\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 25. Waarschuwing als een fysieke component te warm is
      - alert: HostPhysicalComponentTooHot
        expr: node_hwmon_temp_celsius > node_hwmon_temp_max_celsius
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Host physical component too hot (instance {{ $labels.instance }})
          description: "Physical hardware component too hot\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 26. Kritieke waarschuwing als een overtemperature alarm is geactiveerd
      - alert: HostNodeOvertemperatureAlarm
        expr: ((node_hwmon_temp_crit_alarm_celsius == 1) or (node_hwmon_temp_alarm == 1))
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Host node overtemperature alarm (instance {{ $labels.instance }})
          description: "Physical node temperature alarm triggered\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 27. Kritieke waarschuwing als software RAID onvoldoende schijven heeft
      - alert: HostSoftwareRaidInsufficientDrives
        expr: ((node_md_disks_required - on(device, instance) node_md_disks{state="active"}) > 0)
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Host software RAID insufficient drives (instance {{ $labels.instance }})
          description: "MD RAID array {{ $labels.device }} on {{ $labels.instance }} has insufficient drives remaining.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 28. Waarschuwing als een software RAID schijf is uitgevallen
      - alert: HostSoftwareRaidDiskFailure
        expr: (node_md_disks{state="failed"} > 0)
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host software RAID disk failure (instance {{ $labels.instance }})
          description: "MD RAID array {{ $labels.device }} on {{ $labels.instance }} needs attention.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 29. Info alert als de kernel versie is gewijzigd
      - alert: HostKernelVersionDeviations
        expr: changes(node_uname_info[1h]) > 0
        for: 0m
        labels:
          severity: info
        annotations:
          summary: Host kernel version deviations (instance {{ $labels.instance }})
          description: "Kernel version for {{ $labels.instance }} has changed.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 30. Waarschuwing als er een OOM kill is opgetreden
      - alert: HostOomKillDetected
        expr: (increase(node_vmstat_oom_kill[1m]) > 0)
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Host OOM kill detected (instance {{ $labels.instance }})
          description: "OOM kill detected\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 31. Info alert als er correctable EDAC errors zijn gedetecteerd
      - alert: HostEdacCorrectableErrorsDetected
        expr: (increase(node_edac_correctable_errors_total[1m]) > 0)
        for: 0m
        labels:
          severity: info
        annotations:
          summary: Host EDAC Correctable Errors detected (instance {{ $labels.instance }})
          description: "Host {{ $labels.instance }} has had {{ printf \"%.0f\" $value }} correctable memory errors reported by EDAC in the last 5 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 32. Waarschuwing als er uncorrectable EDAC errors zijn gedetecteerd
      - alert: HostEdacUncorrectableErrorsDetected
        expr: (node_edac_uncorrectable_errors_total > 0)
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Host EDAC Uncorrectable Errors detected (instance {{ $labels.instance }})
          description: "Host {{ $labels.instance }} has had {{ printf \"%.0f\" $value }} uncorrectable memory errors reported by EDAC in the last 5 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 33. Waarschuwing bij hoge host network transmit errors (> 1% errors)
      - alert: HostNetworkReceiveErrors
        expr: (rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01)
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host Network Receive Errors (instance {{ $labels.instance }})
          description: "Host {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 34. Waarschuwing bij hoge host network receive errors (> 1% errors)
      - alert: HostNetworkTransmitErrors
        expr: (rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01)
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host Network Transmit Errors (instance {{ $labels.instance }})
          description: "Host {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 35. Waarschuwing als een network bond is gedegradeerd
      - alert: HostNetworkBondDegraded
        expr: ((node_bonding_active - node_bonding_slaves) != 0)
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host Network Bond Degraded (instance {{ $labels.instance }})
          description: "Bond \"{{ $labels.device }}\" degraded on \"{{ $labels.instance }}\".\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 36. Waarschuwing als conntrack limiet bijna is bereikt (> 80%)
      - alert: HostConntrackLimit
        expr: (node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 0.8)
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Host conntrack limit (instance {{ $labels.instance }})
          description: "The number of conntrack is approaching limit\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 37. Waarschuwing als host clock skew is gedetecteerd (> 50ms afwijking)
      - alert: HostClockSkew
        expr: ((node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0))
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: Host clock skew (instance {{ $labels.instance }})
          description: "Clock skew detected. Clock is out of sync. Ensure NTP is configured correctly on this host.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      # 38. Waarschuwing als host clock niet synchroniseert (max error >=16s)
      - alert: HostClockNotSynchronising
        expr: (min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16)
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host clock not synchronising (instance {{ $labels.instance }})
          description: "Clock not synchronising. Ensure NTP is configured on this host.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

  - name: podman_exporter_alerts
    rules:
      # 39. Alarm als een container stopt (behalve als je hem zelf stopt, 
        # dit kijkt naar containers die 'niet running' zijn maar wel bestaan).
        # De podman-exporter gebruikt integers voor status:
        # -1=unknown, 0=created, 1=initialized, 2=running,
        # 3=stopped, 4=paused, 5=exited, 6=removing, 7=stopping
      - alert: PodmanContainerDown
        expr: podman_container_state != 2
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Container {{ $labels.name }} draait niet"
          description: "Container {{ $labels.name }} heeft status code {{ $value }} (niet running)."

      # 40. Podman Exporter down alert
      - alert: PodmanExporterDown
        expr: up{job="podman-exporter"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Podman Exporter down"
          description: "Kan geen metrics ophalen van de Podman socket."

      # ---------------------------------------------------------
      # 1. Container Crashes & Status
      # ---------------------------------------------------------
      - alert: PodmanContainerStopped
        # Alarm als een container NIET draait, tenzij handmatig gestopt.
        # Filtert op exit codes die niet 0 (succes) of 137/143 (SIGTERM/SIGKILL door user) zijn.
        expr: podman_container_state{state="exited"} == 1 and podman_container_exit_code != 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.name }} is gecrasht"
          description: "Container {{ $labels.name }} (Image: {{ $labels.image }}) is gestopt met exit code {{ $value }}."

      - alert: PodmanContainerRestartLoop
        # Alarm als een container meer dan 2x herstart in 5 minuten
        expr: increase(podman_container_restart_count[5m]) > 2
        labels:
          severity: critical
        annotations:
          summary: "Container {{ $labels.name }} herstart constant"
          description: "Mogelijke CrashLoop: Container is {{ $value }} keer herstart in de laatste 5 minuten."

      - alert: PodmanContainerUnhealthy
        # Alleen relevant als je containers een HEALTHCHECK hebben gedefinieerd in hun Dockerfile/Containerfile
        # health_status: 0=healthy, 1=unhealthy, 2=starting
        expr: podman_container_health_status == 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Container {{ $labels.name }} is unhealthy"
          description: "De healthcheck van container {{ $labels.name }} faalt."

      # ---------------------------------------------------------
      # 2. Performance & Resources
      # ---------------------------------------------------------
      - alert: PodmanContainerHighCpu
        # Alarm als een container meer dan 80% van 1 CPU core gebruikt (pas drempel aan naar wens)
        # We normaliseren dit over 5 minuten om pieken te negeren.
        expr: rate(podman_container_cpu_usage_seconds_total[5m]) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Hoog CPU gebruik: {{ $labels.name }}"
          description: "Container {{ $labels.name }} gebruikt veel CPU ({{ $value }}%)."

      - alert: PodmanContainerHighMemory
        # Alarm als een container meer dan 1GB RAM gebruikt (pas drempel aan naar wens)
        # Podman exporter geeft bytes terug. 1e9 = 1GB.
        expr: podman_container_memory_usage_bytes > 1e9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Hoog Geheugen gebruik: {{ $labels.name }}"
          description: "Container {{ $labels.name }} gebruikt meer dan 1GB RAM ({{ $value | humanize1024 }})."

      - alert: PodmanContainerOOMKilled
        # Werkt alleen als podman-exporter toegang heeft tot OOM events
        expr: increase(podman_container_oom_killed_count[5m]) > 0
        labels:
          severity: critical
        annotations:
          summary: "Container {{ $labels.name }} gedood door OOM"
          description: "De container is door het systeem gestopt wegens geheugentekort (Out Of Memory)."

  - name: watchdog_alerts
    rules:
      - alert: Watchdog
        expr: vector(1)
        labels:
          severity: none
        annotations:
          summary: "Prometheus werkt en stuurt alerts naar Alertmanager"
          description: "Dit is een heartbeat alert die altijd 'firing' is."

  - name: alertmanager_alerts
    rules:
      # --- ALERTMANAGER ---
      - alert: AlertmanagerConfigInconsistent
        expr: count(alertmanager_config_hash) by (cluster) > 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Alertmanager configuratie inconsistent"
          description: "Verschillende alertmanagers hebben verschillende configs."

      - alert: AlertmanagerFailedReload
        expr: alertmanager_config_last_reload_successful == 0
        labels:
          severity: warning
        annotations:
          summary: "Alertmanager reload mislukt"
          description: "De laatste configuratiewijziging kon niet geladen worden."

  - name: loki_alerts
    rules:
      # --- LOKI (Omdat Loki soms metrics dropt bij heavy load) ---
      - alert: LokiRequestErrors
        # Check op 5xx errors in de Loki API
        expr: 100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m])) by (namespace, job, route) / sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Loki geeft veel errors"
          description: "Meer dan 10% van de requests naar Loki falen."

      - alert: LokiPanics
        expr: sum(increase(loki_panic_total[10m])) > 0
        labels:
          severity: critical
        annotations:
          summary: "Loki is gecrasht (Panic)"
          description: "Loki heeft een panic geregistreerd in de logs/metrics."

      - alert: LokiProcessTooManyRestarts
        expr: changes(process_start_time_seconds{job=~".*loki.*"}[15m]) > 2
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Loki process too many restarts (instance {{ $labels.instance }})
          description: "A loki process had too many restarts (target {{ $labels.instance }})\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: LokiRequestErrors
        expr: 100 * sum(rate(loki_request_duration_seconds_count{status_code=~"5.."}[1m])) by (namespace, job, route) / sum(rate(loki_request_duration_seconds_count[1m])) by (namespace, job, route) > 10
        for: 15m
        labels:
          severity: critical
        annotations:
          summary: Loki request errors (instance {{ $labels.instance }})
          description: "The {{ $labels.job }} and {{ $labels.route }} are experiencing errors\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: LokiRequestPanic
        expr: sum(increase(loki_panic_total[10m])) by (namespace, job) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Loki request panic (instance {{ $labels.instance }})
          description: "The {{ $labels.job }} is experiencing {{ printf \"%.2f\" $value }}% increase of panics\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: LokiRequestLatency
        expr: (histogram_quantile(0.99, sum(rate(loki_request_duration_seconds_bucket{route!~"(?i).*tail.*"}[5m])) by (le)))  > 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: Loki request latency (instance {{ $labels.instance }})
          description: "The {{ $labels.job }} {{ $labels.route }} is experiencing {{ printf \"%.2f\" $value }}s 99th percentile latency\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

  - name: grafana_alerts
    rules:
      # --- GRAFANA ---
      - alert: GrafanaAlertingError
        expr: rate(grafana_alerting_schedule_query_errors_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Grafana Alerting Fouten"
          description: "Grafana kan zijn eigen alerts niet evalueren."

  - name: alloy_alerts
    rules:
      # --- ALLOY ---
      - alert: AlloyUnhealthy
        # Alloy exposeert 'alloy_component_health' (0=healthy, 1=unhealthy)
        expr: alloy_component_health == 1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Alloy component ongezond"
          description: "Component {{ $labels.component_id }} in Alloy rapporteert een error."

      - alert: GrafanaAlloyServiceDown
        expr: count by (instance) (alloy_build_info) unless count by (instance) (alloy_build_info offset 2m)  
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Grafana Alloy service down (instance {{ $labels.instance }})
          description: "Alloy on (instance {{ $labels.instance }}) is not responding or has stopped running.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

  - name: blackbox_alerts
    rules:
      - alert: ServiceHealthCheckFailed
        # probe_success is 1 (gelukt) of 0 (mislukt)
        expr: probe_success == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Health Check Failed: {{ $labels.instance }}"
          description: "De HTTP probe naar {{ $labels.instance }} is mislukt. Service is mogelijk down of reageert traag."

      - alert: ServiceSlowResponse
        # Waarschuwing als response trager is dan 1 seconde
        expr: probe_duration_seconds > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Trage response: {{ $labels.instance }}"
          description: "De HTTP probe naar {{ $labels.instance }} duurt langer dan 1 seconde (huidige waarde: {{ $value | humanizeDuration }})."

  - name: opentelemetrycollector_alerts
    rules:
      - alert: OpentelemetryCollectorDown
        expr: up{job=~".*otel.*collector.*"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: OpenTelemetry Collector down (instance {{ $labels.instance }})
          description: "OpenTelemetry Collector instance has disappeared or is not being scraped\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: OpentelemetryCollectorReceiverRefusedSpans
        expr: rate(otelcol_receiver_refused_spans[5m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: OpenTelemetry Collector receiver refused spans (instance {{ $labels.instance }})
          description: "OpenTelemetry Collector is refusing spans on {{ $labels.receiver }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: OpentelemetryCollectorReceiverRefusedMetricPoints
        expr: rate(otelcol_receiver_refused_metric_points[5m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: OpenTelemetry Collector receiver refused metric points (instance {{ $labels.instance }})
          description: "OpenTelemetry Collector is refusing metric points on {{ $labels.receiver }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: OpentelemetryCollectorReceiverRefusedLogRecords
        expr: rate(otelcol_receiver_refused_log_records[5m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: OpenTelemetry Collector receiver refused log records (instance {{ $labels.instance }})
          description: "OpenTelemetry Collector is refusing log records on {{ $labels.receiver }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: OpentelemetryCollectorExporterFailedSpans
        expr: rate(otelcol_exporter_send_failed_spans[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: OpenTelemetry Collector exporter failed spans (instance {{ $labels.instance }})
          description: "OpenTelemetry Collector failing to send spans via {{ $labels.exporter }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: OpentelemetryCollectorExporterFailedMetricPoints
        expr: rate(otelcol_exporter_send_failed_metric_points[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: OpenTelemetry Collector exporter failed metric points (instance {{ $labels.instance }})
          description: "OpenTelemetry Collector failing to send metric points via {{ $labels.exporter }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: OpentelemetryCollectorExporterFailedLogRecords
        expr: rate(otelcol_exporter_send_failed_log_records[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: OpenTelemetry Collector exporter failed log records (instance {{ $labels.instance }})
          description: "OpenTelemetry Collector failing to send log records via {{ $labels.exporter }}\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: OpentelemetryCollectorExporterQueueNearlyFull
        expr: (otelcol_exporter_queue_size / on(instance, job, exporter) otelcol_exporter_queue_capacity) > 0.8 and otelcol_exporter_queue_capacity > 0
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: OpenTelemetry Collector exporter queue nearly full (instance {{ $labels.instance }})
          description: "OpenTelemetry Collector exporter {{ $labels.exporter }} queue is over 80% full\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: OpentelemetryCollectorProcessorRefusedSpans
        expr: rate(otelcol_processor_refused_spans[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: OpenTelemetry Collector processor refused spans (instance {{ $labels.instance }})
          description: "OpenTelemetry Collector processor {{ $labels.processor }} is refusing spans, likely due to backpressure\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: OpentelemetryCollectorProcessorRefusedMetricPoints
        expr: rate(otelcol_processor_refused_metric_points[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: OpenTelemetry Collector processor refused metric points (instance {{ $labels.instance }})
          description: "OpenTelemetry Collector processor {{ $labels.processor }} is refusing metric points, likely due to backpressure\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: OpentelemetryCollectorHighMemoryUsage
        expr: (otelcol_process_runtime_heap_alloc_bytes{job=~".*otel.*collector.*"} / on(instance, job) otelcol_process_runtime_total_sys_memory_bytes{job=~".*otel.*collector.*"}) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: OpenTelemetry Collector high memory usage (instance {{ $labels.instance }})
          description: "OpenTelemetry Collector memory usage is above 90%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

      - alert: OpentelemetryCollectorOtlpReceiverErrors
        expr: rate(otelcol_receiver_accepted_spans{receiver=~"otlp"}[5m]) == 0 and rate(otelcol_receiver_refused_spans{receiver=~"otlp"}[5m]) > 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: OpenTelemetry Collector OTLP receiver errors (instance {{ $labels.instance }})
          description: "OpenTelemetry Collector OTLP receiver is completely failing - all spans are being refused\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"

  - name: minio_alerts
    rules:
      - alert: MinioClusterOffline
        expr: minio_cluster_nodes_offline_total > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "MinIO Node Offline"
          description: "Een node in het MinIO cluster is offline."

      - alert: MinioDiskOffline
        expr: minio_cluster_disk_offline_total > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "MinIO Disk Offline"
          description: "Een disk in MinIO is niet bereikbaar."

  - name: tempo_alerts
    rules:
      - alert: HighSpanLatency
        # Alarmeer als 95e percentiel latency > 2s
        expr: histogram_quantile(0.95, sum(rate(traces_spanmetrics_latency_bucket[5m])) by (le, service)) > 2
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Service {{ $labels.service }} is traag"
          description: "De 95e percentiel latency van traces is hoger dan 2s."